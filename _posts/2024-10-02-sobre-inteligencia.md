---
published: true
title: Sobre inteligencia
layout: post
tags: [Inteligencia Artificial, IQ, opinión]
---

Me gustaría tratar de esbozar aquí una discusión que parece que no tiene fin entre los expertos en IA que escucho y leo en las redes sociales. Me refiero a la discusión sobre qué es la inteligencia general, dónde están los humanos, dónde están las máquinas e incluso dónde están los animales si nos pusiéramos todos en una escala.

Lo primero que quiero decir es que yo no soy un experto en IA ni un experto en ciencias cognitivas, solo intento resumir lo que entiendo de leer y escuchar a los expertos en las diferentes materias para tratar de aclarar mi propia opinión.

El segundo punto importante antes de empezar es que no existe un solo tipo de inteligencia, la "inteligencia" tal cual es un concepto demasiado amplio que la mayoría de las personas relacionamos con la capacidad de aprender, relacionar, entender conceptos complejos, resolver problemas, desenvolvernos en situaciones difíciles, etc. De hecho, si nos vamos al campo de la psicología, los llamados tests de inteligencia miden siempre varios aspectos diferentes como la comprensión lectora, el cálculo mental, la creatividad, pensamiento lógico, memoria... De cada una de esas características se hacen una serie de pruebas, que terminan en una nota; el conocido IQ o coeficiente intelectual no es más que una media de esas notas.

Como todas las medias, tiene sus problemas. Porque una persona puede ser brillante en un área y mediocre o mala en otras (y eso nos lleva a las definiciones de talento simple, talento complejo o sobredotación, según si un individuo está por encima del percentil 75-80% en una, varias o todas las áreas...) Pero más o menos es una forma aceptada de medir la inteligencia de una persona alfabetizada (la mayoría de las pruebas son por escrito, con lo que aunque seas un genio del nivel de Einstein o Mozart, si no sabes leer o escribir no puedes hacerlas).

Con todo eso para tener en cuenta como disclaimers, vayamos al meollo del asunto:

## ¿Puede la IA sobrepasar a los humanos?

Para mí esta es la clave, y quiero esbozar primero los dos bandos principales que veo entre los expertos:

En el primer grupo, a los que llamaré "humanistas" por falta de una palabra mejor, estarían los expertos en AI que consideran que los humanos somos el pináculo de la inteligencia, que la evolución nos ha dotado con una capacidad de abstracción, razonamiento y meta-cognición que no es alcanzable por medios artificiales.

Las más conocidas defensoras de esta postura son **Timnit Grebu y Emily M. Bender**, dos de las autoras del famoso paper sobre los ["Stocastic parrots"][1] Otros que podríamos incluir aunque no sean tan radicales podrían ser **Gary Marcus o Margaret Mitchell, o incluso Yann LeCun**. Cada uno tiene su propia visión del problema, pero todos más o menos coinciden en que los LLMs no se parecen a la ingeligencia humana y nunca lo harán. Algunos de estos expertos pueden llegar a conceder que si dotáramos a un robot de inteligencia, y agencia y le diéramos sensores parecidos a los de los humanos, y suficiente capacidad de cómputo y memoria... ([grounded intelligence][2]) Tal vez, y solo tal vez, podría aprender a ser una especie de simulacro de humano.

Es importante notar que cuando estos expertos hablan de la imposibilidad de que las máquinas alcancen la inteligencia humana, no se refieren necesariamente a un "alma" o consciencia inefable, sino a diferencias fundamentales en cómo procesan la información los sistemas actuales de IA versus cómo lo hacen los cerebros biológicos. Su argumento principal es que el reconocimiento de patrones estadísticos, por muy sofisticado que sea, es cualitativamente diferente de la _comprensión genuina_ que exhiben los humanos.

![Meme from I robot where Will Smith ask a robot if they can create a symfony and the robot asks back: can you?](/public/img/i_robot_meme.png)

En el otro lado del espectro están aquellos que dicen que los humanos no somos más que un montón de heurísticas y sesgos cognitivos pegados y revueltos por la evolución a base de ensayo y error y que más que el pináculo de la inteligencia somos el mínimo viable para crear y mantener una cultura común. Este lado incluye a investigadores y emprendedores conocidos como **Sam Altman, Dario Amodei, Ilya Sutskever o Geoffrey Hinton**. Aunque normalmente no lo argumenten de la misma forma, su conclusión es que a mayor escala y mayor cantidad de datos, estos sistemas se harán más inteligentes y podrán realizar tareas más complejas, mucho más complejas de lo que cualquier persona podrá entender y a una velocidad mucho mayor.

Para ellos, la cultura y las relaciones entre conceptos son la verdadera inteligencia que nos propulsa hacia adelante como especie y nos permite crear sociedades, instituciones, compañías y demás agrupaciones y expresiones culturales que se perpetúan a sí mismas y que culminan en la IA. La IA no es más que la expansión de esa inteligencia colectiva, de esa cultura que se ha perdurado y que nos sobrevive a todos como humanos y por tanto puede alcanzar cotas mucho más altas si le damos entidad. A estos es a los que podríamos llamar trans-humanistas o doomers, según si piensan que la IA es una bendición o un riesgo existencial. Este es el grupo al que los medios dan más altavoz y el que escribe más libros, crea más hype y hace más lobby, pero nada de eso lo hace tener razón, y más cuando dentro del propio grupo hay tanta diferencia de opiniones en cuanto a lo que la IA supone para el progreso. Pero ese es un tema que no me interesa tanto como el hecho de que todos consideran que la IA será muy superior en inteligencia al ser humano.

![Image from Ex Machina where one characters talks about IA looking at humans like we look at australopitecus](/public/img/ex_machina_meme.png)

Para mí estamos ante dos campos diametralmente opuestos que, sin embargo, nunca lo externalizan así. Ambos campos hablan como si su punto de vista fuera la única realidad posible y como si el otro estuviera también en esa realidad, pero no la entendiera del todo bien...

Mi pequeño intento de reflexión, por tanto, es el de exponer estos puntos de vista y tratar de analizarlos desde fuera, para ver dónde se sitúan en realidad.

## ¿Cómo de inteligentes somos en conjunto los humanos y hasta dónde pueden llegar las máquinas?

Empecemos por el rango de inteligencias humanas. Porque para mí no existe una inteligencia humana, como ambos bandos asumen, sino muchas. Incluso dentro del, más o menos socialmente aceptado IQ, tenemos todo un rango donde las personas con un IQ de 70 o inferior se consideran dependientes ya que no pueden llevar una vida normal por sí solas, hasta las personas con IQ 150 o superior, que se consideran genios por encima del 99.9% de la población. (Aún así, estos individuos son, teóricamente, uno de cada 1000, con lo que solo en España debería haber varias decenas de miles de ellos...) Pero, claro está, no es todo el IQ. La situación socioeconómica y cultural, sus conexiones sociales y la suerte de ese individuo tienen mucho que decir en su futuro. Y vaya por delante, no siente más, ni es más, una persona con más IQ ni siente menos ni es menos una persona con menos IQ.

![alt text](/public/img/iq_chart.png)

Además, como decía en el disclaimer del principio, uno puede ser un genio en matemáticas y a la vez ser disléxico, o sordo. Puedes tener oído perfecto pero nunca aprender a tocar un instrumento. Puedes ser muy bueno con los problemas lógicos y malo con los espaciales, etc., etc. La inteligencia tiene un millar de componentes diferentes que aún estamos aprendiendo a distinguir y que influyen, junto con la suerte y el entorno, en si una persona acaba siendo un premio Nobel o otro currito más.

![Terence Tao has an IQ of 224](/public/img/terence_tao_iq.png)
Imagen que muestra dónde cae el [IQ de Terence Tao dentro de la gráfica de IQ][3]

Pero ¿entonces qué? ¿Somos "el tope" de la inteligencia? Visto toda esta variación no parece que sea así. Parece más bien que hay muchas inteligencias y que unas serán mejores para ciertas tareas que otras.

<iframe width="560" height="315" src="https://www.youtube.com/embed/zsXP8qeFF6A?si=8CEMQGEaYFrtgDgK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Los animales no encajan en nuestros tests de IQ, pero sí demuestran inteligencia de muchas formas [cada vez mejor documentadas][4]. Hay registros de cuervos resolviendo rompecabezas, de chimpancés recordando grupos de números en un abrir y cerrar de ojos (en el video de arriba), de orcas organizando planes de caza complejos, simios usando herramientas, lenguajes y dialectos animales, y animales "bilingües" (que son capaces de entender dos dialectos de regiones diferentes cuando se les muda de una a otra)... Y eso son solo inteligencias que nosotros podemos apreciar. Cada vez está más claro que los animales no tienen el cerebro solo para llevarlo de un sitio para otro o para gestionar sus órganos internos, sino que muchos de ellos son capaces de recordar, planificar, reconocerse, orientarse o coordinarse a niveles que no comprendemos.

Todo esto pinta una foto de la inteligencia mucho más rica y variada. La inteligencia no es un monolito, es más bien un conjunto de habilidades, agudezas, capacidades, conocimientos y competencias. Un chimpacé tiene más memoria de trabajo que nosotros y más agilidad, pero los chimpancés no han podido crear culturas y acumular conocimiento como nosotros. Una calculadora es órdenes de magnitud más rápida y fiable que el mejor ser humano haciendo aritmética o cálculo diferencial. Una base de datos es mucho más fiable, repetible y transparente que la memoria humana. Pero ambos carecen totalmente de agencia o de capacidad de interactuar con el mundo. Pero entonces, ¿qué puede hacer la IA? ¿Dónde cae en todo este espectro? ¿Hasta dónde llegará?

Ese es el siguiente problema. Voy a intentar reflejar en un par de gráficos lo que yo veo cuando los miembros de uno y otro bando hablan del tema.

Para mí, los humanistas ven algo así:

![Visión humanista del espectro de la inteligencia](/public/img/humanista.jpg)

Para ellos la IA claramente nunca alcanzará a los humanos. Apenas es más inteligente que un reptil o un pájaro, aún no está al nivel de otros mamíferos, y quedan décadas, si no siglos, para que una máquina pueda parecerse suficientemente a un humano como para sacar el debate a la palestra. **Lo que estamos haciendo con este debate es distraer al público de los problemas importantes actuales del aprendizaje automático, como los sesgos, la propiedad intelectual de los datos de entrenamiento o las personas discriminadas por estos sistemas o por no tener acceso a ellos** (todas preocupaciones totalmente legítimas pero que no son el foco de esta reflexión).

Mientras que los trans-humanistas y doomers ven algo así:

![Visión doomer del espectro de la inteligencia](/public/img/doomer.jpg)

Para ellos **es evidente que la IA más temprano que tarde superará a las personas y entonces no habrá retorno. En poco tiempo alcanzará velocidad de escape, no entenderemos lo que hace ni sus motivos para hacerlo y estaremos bajo su merced o su bendición eterna**, según el sub-bando.

De hecho, después de hacer ese gráfico, encontré este otro, que es un gráfico de Leopold Aschenbrenner, uno de los niños prodigios de OpenAI que ahora está creando su propia empresa sobre superinteligencia:

![Gráfico extrapolando la explosión de inteligencia con márgenes de error](/public/img/intelligence_explosion.png)

En su ensayo ["La década siguiente"](https://situational-awareness.ai/from-agi-to-superintelligence/) Aschenbrenner asegura que la ASI es inevitable y que la escalada tanto en energía como en dinero y recursos para conseguirla seguirá imparable sin darse con ningún muro físico, económico ni social...

## ¿Quién tiene razón?

Solo el tiempo lo dirá. Pero después de ver toda la variabilidad que hay en los humanos y los animales, yo me inclino a pensar que la gráfica, en realidad es algo así:

![Visión intermedia](/public/img/middle_ground.jpg)

Para mí la Inteligencia Artificial General, o por sus siglas en inglés: AGI, será algo que estará en medio de todo ese espectro de inteligencias que exhibimos los humanos. Habrá AGIs que no podrán ser "autónomas" aunque tengan añgun tipo de memoria persistente y agencia. Como niños a los que hay que cuidar.

Incluso podríamos argumentar que sistemas actuales como GPT-4 o Claude 3.5 ya demuestran capacidades que superan a los humanos en ciertas áreas específicas (como el procesamiento de lenguaje o la recuperación de información), mientras que siguen siendo muy inferiores en otras (como el razonamiento causal o la comprensión del mundo físico). Los LLMs tienen, sin embargo, muchos problemas y están lejos de tener la capacidad ejecutiva de un humano. No se les da bien planificar o ejecutar planes, no se les da bien razonar ni saber cuando realmente están respondiendo algo _"simplemente porque suena bien"_. Todo esto se está intentando paliar con arquitecturas que incluyen memorias externas, reintentos, control del output para evitar respuestas inaproviadas o control del input para evitar jail-break. Pero esto indica que hay mucho trabajo por hacer. Personalmente creo que el camino a la ASI es mucho más largo de lo que se presume ahora mismo y que estaremos unos cuantos años en este limbo de AIs muy potentes en algunos aspectos y muy torpes en otros, que supondrán grandes mejoras en muchos campos como estamos viendo con AlphaFold o Copilot, pero no serán AGIs.

Lo estamos viendo con los coches autónomos: ya conducen miles de kilómetros al día, pero sólo en ciudades concretas y en condiciones concretas, expandir esa red donde pueden operar con seguridad es un proceso lento porque cada ciudad y cada pais tiene diferente contexto.

El propio Sam Altman en su famoso ensayo ["The intelligence Age"](https://ia.samaltman.com/) hablaba de ASI en "miles de días" una medida tan imprecisa como poco habitual. Miles de días podrían ser 3650 días (10 años), o podrían ser 9000 (~30 años) este ensayo debería haber _enfriado_ las espectativas del mercado enormemente, pero no fue así, grandes empresas siguen gastando miles de millones con la esperanza de que si siguen aumentando el tamaño de los modelos y los datasets los modelos seguirán mejorando sin tocar techo.

La visión humanista no se equivoca en que los millones de años de evolución han conseguido sistemas increiblemente complejos y sofisticados. La inteligencia es una ventaja evolutiva. Es lo que los doomers llaman un _fin instrumental convergente_: ser más inteligente es útil para conseguir muchas otras cosas, por lo tanto sea lo que sea que necesitas, ser más inteligente seguramente te ayude. Esto es lo que hizo crecer el cerebro de los hominidos hasta donde nos encontramos ahora: la capacidad de coordinarnos, de planificar, de rastrear una presa o tenderle una trampa, de comunicar conceptos abstractos en forma de palabras y finalmente la capacidad de crear una cultura común que perdure durante generaciones fue una ventaja sobre los otros animales y los otros homínidos. El problema de los humanistas es pensar que nosotros somos el tope o que la IA nunca podrá llegar a donde estamos y sobrepasarnos en muchos aspectos.

La realidad es que la inteligencia, tanto humana como artificial, es multidimensional, multisensorial y contextual. Un sistema puede ser extraordinariamente capaz en un dominio mientras es básico o incompetente en otro. Por eso me parece tan chocante la visión tanto de uno como el otro campo y no entiendo esta batalla argumentativa que no empieza por reconocer que es reduccionista al asumir que todos los humanos tenemos un sólo nivel de inteligencia y que todos estamos de acuerdo en lo que significa.

[1]: https://dl.acm.org/doi/10.1145/3442188.3445922
[2]: https://arxiv.org/abs/2210.13589
[3]: https://check-iq.org/celebrity-iq/terence-tao-iq
[4]: https://www.ultimatekilimanjaro.com/the-15-smartest-animals-in-the-world/
