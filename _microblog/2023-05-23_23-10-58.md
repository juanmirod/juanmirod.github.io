---
published: true
layout: micro
---

Es curioso cómo gpt3.5 se equivoca en pequeños detalles de una implementación mientras la solución general está bien.

Casi parece que esté imitando una respuesta humana donde la persona comete errores en su ejemplo, como una respuesta de stackoverflow que está bien,
pero quien sea no la ha probado y tiene errores y cuando el OP lo dice al final se corrige y acaba siendo la solución que funciona...

Hoy lo escuché también  en un video de MLST, ¿es que se equivoca porque realmente no tiene un modelo de lo que está haciendo o es que su modelo
incluye errores como parte de las respuestas habituales que damos los humanos? ¿Podríamos de alguna forma con el prompt indicarle que no cometa errores?
